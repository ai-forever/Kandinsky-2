params_path:
drop_first_layer: false
clip_name: ViT-L/14
num_epochs: 2
save_every: 1000
save_name: model
save_path: test
inpainting: false
device: cuda
freeze:
  freeze_resblocks: true
  freeze_attention: false
model_config:
  version: '2.1'
  image_size: 64
  num_channels: 384
  num_res_blocks: 3
  channel_mult: ''
  num_heads: 1
  num_head_channels: 64
  num_heads_upsample: -1
  attention_resolutions: 32,16,8
  dropout: 0
  model_dim: 768
  use_scale_shift_norm: true
  resblock_updown: true
  use_fp16: false
  cache_text_emb: false
  text_encoder_in_dim1: 1024
  text_encoder_in_dim2: 768
  image_encoder_in_dim: 768
  num_image_embs: 10
  pooling_type: from_model
  in_channels: 4
  out_channels: 8
  inpainting: false
  up: false

  
diffusion_config:
  learn_sigma: true
  sigma_small: false
  steps: 1000
  noise_schedule: linear
  timestep_respacing: ''
  use_kl: false
  predict_xstart: false
  rescale_timesteps: true
  rescale_learned_sigmas: true
  linear_start: 0.00085
  linear_end: 0.012

optim_params:
    name: transformers.Adafactor
    params:
      lr: 0.000005
      weight_decay: 0
      scale_parameter: false
      relative_step: false
image_enc_params:
    name: MOVQ
    scale: 1
    ckpt_path:
    params:
        embed_dim: 4
        n_embed: 16384
        ddconfig:
          double_z: false
          z_channels: 4
          resolution: 256
          in_channels: 3
          out_ch: 3
          ch: 128
          ch_mult:
          - 1
          - 2
          - 2
          - 4
          num_res_blocks: 2
          attn_resolutions:
          - 32
          dropout: 0.0

text_enc_params:
    model_path: 
    model_name: multiclip
    in_features: 1024
    out_features: 768
        
data:
  train:
    df_path: 
    image_size: 768
    tokenizer_name:
    clip_image_size: 224
    drop_text_prob: 0.5
    drop_image_prob: 0.1
    seq_len: 77
    batch_size: 1
    shuffle: true
    num_workers: 4
